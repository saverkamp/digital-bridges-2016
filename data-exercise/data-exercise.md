##Mapping Correspondence Data Prep Exercise

For this exercise, we will examine a successful digital humanities visualization, [Mapping the Republic of Letters](http://republicofletters.stanford.edu/), and given datasets for five 19th century American authors, strategize the necessary data cleanup, normalization, and enrichment necessary to produce a similar visualization. The Republic of letters consists of a number of individual cases studies each created by teams of different humanities collaborators and each producing a visualization to address specific research questions. We will look at a visualization produced in collaboration with the [Tooling up for Digital Humanities project](http://toolingup.stanford.edu/) at the Stanford University [Center for Spatial and Textual Analysis](http://cesta.stanford.edu/).

Visualization: http://web.stanford.edu/group/toolingup/rplviz/rplviz.swf  
About: http://www.stanford.edu/group/toolingup/rplviz/  
NEH Humanities article: http://www.neh.gov/humanities/2013/novemberdecember/feature/mapping-the-republic-letters   

A map of the correspondence between a number of related people can help reveal patterns not evident through close reading. We will consider the problem of creating a similar visualization based on correspondence data from five collections of related 19th century American authors held at the New York Public Library.

Datasets are included in the [data-exercise](http://github.com/saverkamp/digital-bridges/data-exercise) folder of this project along with a README file describing the source of the data and the fields included.

*STEP 1*: Download the data  
- At the root of this github repository, click the green button that says "Clone or Download," and download the zip file.
- Extract the contents of the zip file to a convenience place on your computer

*STEP 2*: Explore the visualization  
- _Questions_: What is being visualized? What data points are necessary for this visualization? What data points in my datasets can I ignore? How well does the scope of my data support the purpose of my visualization? How will I account for this?

*STEP 3*: Explore your data  
- Open your datasets using the tool of your choice (Excel will work fine) to get familiar with the data included in the datasets.
- Read the information in the included data documentation to learn about what is included in the data.
- _Questions_: What does your data show? What do you know about this data? Where can you get more information? 

*STEP 4*: Define your destination data  
- Create your crosswalk in the format of your choice (a table or spreadsheet works well). For each necessary data point, record what form you think the data will need to take. You may find it helpful to write plain language rules to describe these formats, like "Dates should always be present and in yyyy format." 
- Questions_: Is this data point required for my visualization? Will it make sense if some values are missing? How many instances of each data point may be present?

*STEP 5*: Strategize the path to your destination data  
- Compare your datasets to the destination data formats you created. For each data point, determine what steps you need to take to clean up or enhance the data to get it into the format necessary to power your visualization. Don't worry yet about what tools you will use to clean your data and how you will do it--just consider what needs to be done.
- _Questions_: How consistent is my data across datasets? What is missing? How can I fill in missing values? What will I do about uncertainties or fuzziness? How will I document/explain missing data to my audience? Will I need to clean or enhance this data in a certain order? Will some tasks need to be completed before others? 

